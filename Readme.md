# ğŸ± Real-Time Object Detection & Robot Control

This project implements **real-time object detection** and **autonomous robot control** using a **Raspberry Pi 5** and **deep learning**.  
Our main goal: Enable a Fischertechnik robot to detect and chase a specific object â€“ in this case, a **cat** â€“ using computer vision and control engineering.

---

## ğŸ“Œ Overview

- **Object Detection Model:** YOLOv8n (fine-tuned for single-class cat detection)
- **Hardware:** Raspberry Pi 5, webcam, Fischertechnik robot
- **Inference Engine:** NCNN (Tencent) â€“ optimized for ARM-based edge devices
- **Control System:**  
  - Lateral motion â€“ **PD Controller** (keeps object centered in camera frame)  
  - Longitudinal motion â€“ **P Controller** (maintains desired distance)
- **Dataset:** Custom **mega-dataset of cats**, balanced with non-cat images to avoid false positives.

---

## ğŸ—‚ Dataset Generation

We created a **17,588-image** dataset by combining:
- COCO Dataset
- Roboflow Cats Dataset
- Custom collected cat images

**Key steps:**
1. **Balancing Classes** â€“ Equal ratio of cat vs. non-cat images.
2. **Diversity** â€“ Multiple cat breeds and backgrounds.
3. **Augmentation Techniques** â€“ Flips, rotations, brightness/contrast, HSV adjustments.
4. **Annotation** â€“ YOLOv8n-assisted labeling.

---

## ğŸ§  Model Selection & Training

We evaluated YOLOv5, YOLOv8n, SSD, Faster R-CNN, EfficientDet, and RetinaNet.  
**YOLOv8n** was chosen for:
- High inference speed on Raspberry Pi
- Small model size
- Easy export to NCNN format

**Training strategy:**
- Freeze Backbone & Neck (first 21 layers)
- Fine-tune Head (last layer) for cat classification
- Two-stage training: 30 epochs + 20 epochs

---

## âš¡ Deployment on Raspberry Pi 5

We benchmarked multiple formats for inference speed:

| Format      | Inference Time (ms) | FPS   |
|-------------|--------------------|-------|
| TorchScript | 456                | 1.5â€“2 |
| PyTorch (.pt)| 350                | 2.5â€“3 |
| ONNX        | 300                | 3â€“3.5 |
| **NCNN**    | **110**            | **8â€“9** |

âœ… **NCNN** was selected for **real-time performance**.

---

## ğŸ¯ Control Engineering

### Lateral Motion (PD Controller)
- Keeps the detected object centered in the camera frame.
- Error = horizontal offset between bounding box center and frame center.

### Longitudinal Motion (P Controller)
- Maintains optimal distance using bounding box area.
- Target area = 80% of frame area.

---

## ğŸ”§ Controller Tuning Insights
- **PD Controller:** Low-to-moderate derivative gain offers a good trade-off between responsiveness and stability.
- **P Controller:** Stable convergence but slower response; chosen for simplicity and hardware limitations.

---

## ğŸ“¸ Project Demo
*(Add images or GIFs of the robot in action here)*

---

## ğŸ“š References
1. [Ultralytics YOLO](https://github.com/ultralytics/ultralytics)
2. [Microsoft COCO Dataset](https://cocodataset.org/)
3. [Roboflow Cats Dataset](https://universe.roboflow.com/mohamed-traore-2ekkp/cats-n9b87)

---

## ğŸ† Key Takeaways
- **End-to-end system**: dataset preparation â†’ model fine-tuning â†’ hardware deployment â†’ control loop tuning.
- **Edge AI focus**: Optimized for Raspberry Pi with NCNN.
- **Practical robotics application**: Combining computer vision and control engineering for autonomous chasing behavior.

---

## ğŸ“œ License
This project is released under the MIT License â€“ feel free to use, modify, and share.

---

## ğŸ¤ Contributors
- **Shantanu Shirsath**  
- **Aakash Deshpande**  

University of Siegen

---
